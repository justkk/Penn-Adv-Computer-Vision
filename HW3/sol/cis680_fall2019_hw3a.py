# -*- coding: utf-8 -*-
"""CIS680_Fall2019_HW3a.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oXq0yU7q9WQPaJMpRbttS3Wq1TD1DMhl

# Instructions

Run all the cells in sequential order. Point the PATHS and SAVE_FOLDER_PATH to the storage location.

# Google Drive

This first code block attaches your google drive and makes a folder structure. You only need to run this when a new VM is assigned to you. To get your code as a single python file go through the following menus File->'Download .py'.

This also downloads the 4 files that contain the dataset:


https://drive.google.com/open?id=1uBWazGxSZgWs70JjSWBu-KZwy5sAcxLh

https://drive.google.com/open?id=18Bh2qwVwdDwu7JK_plrnpAz9KjA5gWkv

https://drive.google.com/open?id=1K4eZGmbW0peZvcSRpgJeCSuI6A6PWZYz

https://drive.google.com/open?id=1xIzQrhWrJeid1J8YLFMt8yigFVDA-N8a
"""

from google.colab import drive
drive.mount('/content/gdrive')

import os
from google.colab import drive

# Mount google drive
DRIVE_MOUNT='/content/gdrive'
drive.mount(DRIVE_MOUNT)


# create folder to write data to
CIS680_FOLDER=os.path.join(DRIVE_MOUNT, 'My Drive', 'CIS680_2019')
HOMEWORK_FOLDER=os.path.join(CIS680_FOLDER, 'HW2')
os.makedirs(HOMEWORK_FOLDER, exist_ok=True)

# bootstrap environment into place
from google.colab import auth
auth.authenticate_user()

from googleapiclient.discovery import build
drive_service = build('drive', 'v3')

import io
import os
from googleapiclient.http import MediaIoBaseDownload

def download_file(fn, file_id):
    request = drive_service.files().get_media(fileId=file_id)
    downloaded = io.BytesIO()
    downloader = MediaIoBaseDownload(downloaded, request)
    done = False
    while done is False:
        # _ is a placeholder for a progress object that we ignore.
        # (Our file is small, so we skip reporting progress.)
        _, done = downloader.next_chunk()
    
    downloaded.seek(0)

    folder = fn.split('/')
    if len(folder) > 1:
        os.makedirs(folder[0], exist_ok=True)

    with open(fn, 'wb') as f:
        f.write(downloaded.read())

id_to_fn = {
'1uBWazGxSZgWs70JjSWBu-KZwy5sAcxLh': 'hw3_mycocodata_bboxes_comp_zlib.npy',
'18Bh2qwVwdDwu7JK_plrnpAz9KjA5gWkv': 'hw3_mycocodata_img_comp_zlib.h5',
'1K4eZGmbW0peZvcSRpgJeCSuI6A6PWZYz': 'hw3_mycocodata_labels_comp_zlib.npy',
'1xIzQrhWrJeid1J8YLFMt8yigFVDA-N8a': 'hw3_mycocodata_mask_comp_zlib.h5',
}

# download all files into the vm
for fid, fn in id_to_fn.items():
    print("Downloading %s from %s" % (fn, fid))
    download_file(fn, fid)

"""# Global Vars

These are the global constants used in the code. Change the SAVE_FOLDER_PATH variable to store the outputs.
"""

## Path map to store the paths of input file. 
import torch

PATHS = {}
PATHS['boxes'] = '/content/hw3_mycocodata_bboxes_comp_zlib.npy'
PATHS['labels'] = '/content/hw3_mycocodata_labels_comp_zlib.npy' 
PATHS['images'] = '/content/hw3_mycocodata_img_comp_zlib.h5'
PATHS['masks'] = '/content/hw3_mycocodata_mask_comp_zlib.h5'


COLOR_MAP = {}
COLOR_MAP[-1] = 'r'
COLOR_MAP[0] = 'y'

COLOR_MAP[1] = 'r'
COLOR_MAP[2] = 'g'
COLOR_MAP[3] = 'b'


ALLOCATE_IOU_TH = 0.7
N_ALLOCATE_IOU_TH = 0.3

ABOX_SCALE = 64
ABOX_ASPECT_RATIO = 0.66

IMG_SIZE = [300.0, 400.0]
GRID_R = 18
GRID_C = 25

TRAIN_TEST_RATIO = 0.80

MINI_BATCH_SIZE = 100

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

SAVE_FOLDER_PATH = "./gdrive/My Drive/CIS680_2019/HW3/"

NMS_PROB_LIMIT = 0.5
NMS_IOU_LIMIT = 0.5

## GT to Label assignment
IOU_T = 0.5

"""# Wrapper Classes"""

import math

class BBox():
  def __init__(self, p1, p2, assignment=None):
    
    if assignment is not None:
      self.x, self.y, self.w, self.h = assignment['x'], assignment['y'], assignment['w'], assignment['h']
      self.x1, self.y1, self.x2, self.y2 = assignment['x1'], assignment['y1'], assignment['x2'], assignment['y2']
      return
    
    self.x1, self.x2 = min(p1[0], p2[0]), max(p1[0], p2[0])
    self.y1, self.y2 = min(p1[1], p2[1]), max(p1[1], p2[1])
    
    self.x = (self.x1 + self.x2)* 0.5
    self.y = (self.y1 + self.y2)* 0.5
    
    self.w = self.x2 - self.x1
    self.h = self.y2 - self.y1
  
  
  @staticmethod
  def from_w_h(x, y, w, h):
    x1 = x - w * 0.5
    x2 = x + w * 0.5
    y1 = y - h * 0.5 
    y2 = y + h * 0.5
    
    assignment = {}
    assignment['x'], assignment['y'], assignment['w'], assignment['h'] = x, y, w, h
    assignment['x1'], assignment['y1'], assignment['x2'], assignment['y2'] = x1, y1, x2, y2
    return BBox(None, None, assignment=assignment)
    
  def get_area(self):
    return self.w * self.h
  
  def get_aspect_ratio(self):
    return self.w * 1.0 / self.h

class Label():
    
  def __init__(self, bbox, label, mask, prob=1.0):
    self.label, self.bbox = label, bbox
    self.color = COLOR_MAP[label]
    self.prob = prob
    self.mask = mask
    
    
  @staticmethod
  def label_from_cords(x1, y1, x2, y2, label, mask):
    bbox = BBox((x1, y1), (x2, y2))
    return Label(bbox, label, mask)
  
class RelativeCord():
  
  def __init__(self, gridbox, bbox):
    self.gridbox = gridbox
    self.bbox = bbox
    
    self.x = (self.bbox.x - self.gridbox.x) * 1.0 / self.gridbox.w
    self.y = (self.bbox.y - self.gridbox.y) * 1.0 / self.gridbox.h
    
    self.w = math.log(self.bbox.w * 1.0/ self.gridbox.w)
    self.h = math.log(self.bbox.h * 1.0/ self.gridbox.h)
  
  

class Gridbox():
  
  def __init__(self, anchor_box, row, col, gbox):
    self.row_index = row
    self.col_index = col
    self.anchor_box = anchor_box
    self.gbox = gbox
    self.assign_label = None
  
  def is_cross_boundary_anchor(self):
    
    if self.anchor_box.x1 < 0 or self.anchor_box.x1 > IMG_SIZE[1]:
      return True
    
    if self.anchor_box.y1 < 0 or self.anchor_box.y1 > IMG_SIZE[0]:
      return True
    
    if self.anchor_box.x2 < 0 or self.anchor_box.x2 > IMG_SIZE[1]:
      return True
    
    if self.anchor_box.y2 < 0 or self.anchor_box.y2 > IMG_SIZE[0]:
      return True
    
    return False
      
  def get_feature_vector_training(self):
    
    pc = -1.0
    bx, by, bw, bh = 0.0, 0.0, 0.0, 0.0
    if self.assign_label is not None:
      pc = self.assign_label.prob
      if self.assign_label.bbox is not None:
        rcord = RelativeCord(self.anchor_box, self.assign_label.bbox)
#         rcord = RelativeCord(self.anchor_box, self.anchor_box)
#         print(self.anchor_box.w, self.anchor_box.h)
#         print(self.gbox.w, self.gbox.h)
#         print(self.anchor_box.x1, self.anchor_box.y1)
#         print(self.anchor_box.x2, self.anchor_box.y2)        
        bx, by, bw, bh = rcord.x, rcord.y, rcord.w, rcord.h

    feat = [pc, bx, by, bw, bh] 
    return feat

"""# Utils

Util functions and classes used in the code.
"""

import math
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from PIL import Image
import numpy as np



def IOU_Calculator(bbox1, bbox2):
    
	xa = max(bbox1.x1, bbox2.x1)
	ya = max(bbox1.y1, bbox2.y1)
	xb = min(bbox1.x2, bbox2.x2)
	yb = min(bbox1.y2, bbox2.y2)
 
	inter_area = max(0, xb - xa ) * max(0, yb - ya)
 
	bbox1_area = (bbox1.x2 - bbox1.x1) * (bbox1.y2 - bbox1.y1 )
	bbox2_area = (bbox2.x2 - bbox2.x1) * (bbox2.y2 - bbox2.y1 ) 

	iou = inter_area / float(bbox1_area + bbox2_area - inter_area + 0.000001)
 
	return round(iou,4) 


import random
def random_hex():
  ra = lambda: random.randint(0,255)
  r,g,b = ra(), ra(), ra()
  return '#{:02x}{:02x}{:02x}'.format(r, g, b), r, g, b

def mask_it_image(mask, mask_image, r, g, b):
  
  r_channel = mask_image[:, :, 0]
  g_channel = mask_image[:, :, 1]
  b_channel = mask_image[:, :, 2]
  
  r_channel[mask > 0] = r
  g_channel[mask > 0] = g
  b_channel[mask > 0] = b
  
  return mask_image
  

class Utilities():
  
  ## works for single vector only
  @staticmethod
  def reconstruct_labels(f_v):
    
    grid_data, grid_map = Utilities.grid_generator(IMG_SIZE, GRID_R, GRID_C)
    label_object_list = []
    
    for row in range(GRID_R):
      for col in range(GRID_C):
        pobject, tx, ty, tw, th = f_v[row, col, :]
        
        grid = grid_map[row][col]
        
        xa, ya, wa, ha = grid.anchor_box.x, grid.anchor_box.y, grid.anchor_box.w, grid.anchor_box.h
        
        x = tx * wa  + xa
        y = ty * ha  + ya
        
        w = math.exp(tw) * wa 
        h = math.exp(th) * ha 
        
        x1 = x - w*0.5
        y1 = y - h*0.5
        x2 = x + w*0.5
        y2 = y + h*0.5
        
        if(x1 < 0):
          x1 = 0
        if(x2 < 0):
          x2 = 0
        
        if(x1 > IMG_SIZE[1]):
          x1 = IMG_SIZE[1]
          
        if(x2 > IMG_SIZE[1]):
          x2 = IMG_SIZE[1]
          
          
        if(y1 < 0):
          y1 = 0
        if(y2 < 0):
          y2 = 0
        
        if(y1 > IMG_SIZE[0]):
          y1 = IMG_SIZE[0]
          
        if(y2 > IMG_SIZE[0]):
          y2 = IMG_SIZE[0]
          
        
        
        label_bbox = BBox([x1,y1],[x2,y2])
        mask = np.zeros([int(val) for val in IMG_SIZE])
        
        label = Label(label_bbox, 0, mask, prob=pobject)
        
        if grid.is_cross_boundary_anchor():
          continue
        
        label_object_list.append(label)
        
    return label_object_list
        
  ## Call after allocating grid_data and grid_map
  @staticmethod
  def get_feature_vector_for_image(label_list):
    feature_vector = np.zeros((GRID_R, GRID_C, 5), dtype="float")
    grid_data, grid_map = Utilities.grid_generator(IMG_SIZE, GRID_R, GRID_C)
    Utilities.allocate_grid_to_labels(grid_data, label_list)
    
    for grid in grid_data:
      r = grid.row_index
      c = grid.col_index    
      feat_Vec = grid.get_feature_vector_training()
      feature_vector[r, c, :] = feat_Vec
      
    return feature_vector
  
  
  ## Given a feature vector, sample some positive anchors and negative anchors. 
  @staticmethod
  def random_sample_anchors(feature_vector):
    
    pc_value = feature_vector[:, :, 0]
    
    new_pc_mask = pc_value * 0.0 - 1.0
        
    positive_label_indexes = np.argwhere(pc_value == 1.0)
    negative_label_indexes = np.argwhere(pc_value == 0.0)
    
    np.random.shuffle(positive_label_indexes)
    np.random.shuffle(negative_label_indexes)
    
    positive_limit = min(len(positive_label_indexes), int(MINI_BATCH_SIZE/2))
    negative_limit = min(len(negative_label_indexes), int(MINI_BATCH_SIZE/2))
    
    if positive_limit < MINI_BATCH_SIZE/2 :
      pending = int(MINI_BATCH_SIZE/2) - positive_limit
      negative_allowance = max(0, len(negative_label_indexes) - negative_limit)
      negative_limit += min(negative_allowance, pending)
    
    elif negative_limit < MINI_BATCH_SIZE/2:
      pending = int(MINI_BATCH_SIZE/2) - negative_limit
      positive_allowance = max(0, len(positive_label_indexes) - positive_limit)
      positive_limit += min(positive_allowance, pending)
      
      
    positive_sample_indexes = positive_label_indexes[:positive_limit, :]
    negative_sample_indexes = negative_label_indexes[:negative_limit, :]
    
    
    new_pc_mask[positive_sample_indexes[:, 0], positive_sample_indexes[:, 1]] = 1.0 
    new_pc_mask[negative_sample_indexes[:, 0], negative_sample_indexes[:, 1]] = 0.0
    
    new_feature_map =  feature_vector.copy()
    new_feature_map[:, :, 0] = new_pc_mask
    return new_feature_map
  
    
  
  @staticmethod
  def generate_anchor_box(gbox, scale, aspect_ratio):
    area = pow(scale, 2)
    h = math.sqrt(area * 1.0 / aspect_ratio)
    w = aspect_ratio * h
    x,y = gbox.x, gbox.y 
    anchor_box = BBox.from_w_h(x, y, w, h)
    return anchor_box
  
  @staticmethod
  def grid_generator(img_size, row_size, col_size):
    rows, cols  = img_size[0], img_size[1]
    row_grid, col_grid  = row_size, col_size
    
    p_row, p_col = rows * 1.0 / row_size, cols * 1.0 / col_size

    grid_data = []
    grid_map = {}

    for r in range(row_grid):
      r_start, r_end = r * p_row, (r + 1) * p_row
      if r not in grid_map:
        grid_map[r] = {}
      for c in range(col_grid):
        c_start, c_end = c * p_col, (c+1)* p_col
        gbox = BBox((c_start, r_start), (c_end, r_end))
        anchor_box = Utilities.generate_anchor_box(gbox, ABOX_SCALE, ABOX_ASPECT_RATIO)
        grid_box = Gridbox(anchor_box, r, c, gbox)
        grid_data.append(grid_box)
        grid_map[r][c] = grid_box

    return grid_data, grid_map 
  
  @staticmethod
  def allocate_grid_to_labels(grid_data, label_list):
    
    grid_data_copy = grid_data.copy()
    
    grid_data_copy = [x for x in grid_data_copy if not x.is_cross_boundary_anchor()]
    
    # print(len(grid_data_copy))
    
    ## assign by th
    
    for grid in grid_data_copy:
      atleast_one_label = False
      good_label = None
      prev_iou = -1
      for label in label_list:
        current_iou = IOU_Calculator(grid.anchor_box, label.bbox)
        if current_iou >= ALLOCATE_IOU_TH:
          atleast_one_label = True
          if prev_iou < current_iou:
            good_label = label
            prev_iou = current_iou
      
      if atleast_one_label:
#         print(grid.row_index, grid.col_index, prev_iou)
        grid.assign_label = good_label
    
    grid_data_copy = [x for x in grid_data_copy if x.assign_label is None]
    
    
    ## assign Top IOU
    for label in label_list:
      iou_vals = []
      for grid in grid_data_copy:
        iou_vals.append(IOU_Calculator(grid.anchor_box, label.bbox))
      
      if len(iou_vals) == 0:
        continue
      
      max_iou = max(iou_vals)
      max_iou_grid_indexes = [i for i, x in enumerate(iou_vals) if x == max_iou]
      
#       print(max_iou, max_iou_grid_indexes)
#       print(iou_vals)
      
      for max_iou_grid_index in max_iou_grid_indexes:
        grid_data_copy[max_iou_grid_index].assign_label = label
#         print(label.prob, grid_data_copy[max_iou_grid_index].row_index, grid_data_copy[max_iou_grid_index].col_index)
        
              
      grid_data_copy = [x for i,x in enumerate(grid_data_copy) if i not in max_iou_grid_indexes]
    
    
    ## Assign Negative th
    
    for grid in grid_data_copy:
      all_bad_labels = True
      for label in label_list:
        if IOU_Calculator(grid.anchor_box, label.bbox) >= N_ALLOCATE_IOU_TH:
          all_bad_labels = False
          break
      
      if all_bad_labels:
        grid.assign_label = Label(None, -1, None, prob=0.0)
 
  @staticmethod
  def plot_image(image, label_list, title=None):
    ax = plt.gca()
    image = np.moveaxis(image, 0, 2)
    mask_image = np.zeros(image.shape, dtype=int)
    global_mask = np.zeros((image.shape[0], image.shape[1]) , dtype=float)
    
    for label in label_list:
      rand_string, rand_r, rand_g, rand_b = random_hex()
      
      if label.mask is not None:
        mask = label.mask
        global_mask += mask
        mask_image = mask_it_image(mask, mask_image, rand_r, rand_g, rand_b)

      rect = patches.Rectangle((label.bbox.x1, label.bbox.y1), label.bbox.w, label.bbox.h, linewidth=1, edgecolor=label.color, facecolor='none')
      #rect = patches.Rectangle((60,0), 20, 20, linewidth=1, edgecolor=label.color, facecolor='none')
      ax.add_patch(rect)
    
    if title is not None:
      plt.title(title)
      
    #orig_image = Image.fromarray(image.astype('uint8')).convert("RGBA")
    #mask_image = Image.fromarray(mask_image).convert("RGBA")
    alpha = 0.6
    test_image = (1 - alpha)* image.astype(float) + alpha * mask_image.astype(float)
    test_image = test_image.astype(int)
    exp_global_mask = np.stack([global_mask]*3, axis=2)    
    dis_test_image = np.where(exp_global_mask == 0, image, test_image) 
    
    plt.imshow(dis_test_image)
    plt.show()

"""# Data Loader"""

import numpy as np 
import h5py
import random
random.seed( 30 )


class FullLoader():
  
  def __init__(self, paths):
    self.boxes = np.load(paths['boxes'], allow_pickle=True)
    self.labels = np.load(paths['labels'], allow_pickle=True)
    self.images = h5py.File(paths['images'],'r')['data']
    self.masks = h5py.File(paths['masks'],'r')['data']
    self.sample_size = self.boxes.shape[0]
    self.mask_iindex = {}
        
    #self.build_images()
    
    self.build_mask_iindex()
    
    
    self.sample_indexes = [i for i in range(self.sample_size)]
    random.shuffle(self.sample_indexes)
    
    self.train_len = int(self.sample_size * TRAIN_TEST_RATIO)
    self.test_len = self.sample_size - self.train_len
    
    self.train_indexes = self.sample_indexes[:self.train_len]
    self.test_indexes = self.sample_indexes[self.train_len:]
    
    self.feature_map = {}
    
    
#   def build_images(self):
#     for i in range(self.sample_size):
#       data_image = self.images_pyfile[i]
#       print(data_image.shape)
#       self.images[i, :, :, :] = data_image
      
        
  def build_feature_map(self):
    print(self.sample_size)
    for i in range(self.sample_size):
      print(i)
      image, label_list = self.get_obj_item(i)
      
 
  def build_mask_iindex(self):
    start_index = 0
    for i in range(self.sample_size):
      self.mask_iindex[i] = {}
      box_data = self.boxes[i]
      obj_count = self.boxes[i].shape[0]
      for j in range(obj_count):
        self.mask_iindex[i][j] = start_index
        start_index += 1
  
  def get_item(self, index):
    boxes = self.boxes[index]
    labels = self.labels[index]
    image = self.images[index]
    obj_count = boxes.shape[0]
    masks = np.zeros((obj_count, image.shape[1], image.shape[2]), dtype=float)
    for i in range(obj_count):
      masks[i, : ,:] = self.masks[self.mask_iindex[index][i], :, :]
    return boxes, labels, image.astype(float), masks
  
  def get_obj_item(self, index):
    boxes, labels, image, masks = self.get_item(index)
    label_list = []
    for i in range(boxes.shape[0]):
      bbox = BBox([boxes[i][0], boxes[i][1]], [boxes[i][2], boxes[i][3]])
      label = Label(bbox, labels[i], masks[i, :, :])
      label_list.append(label)
    return image, label_list
  
  def get_feature_item(self, index):
    image, label_list = self.get_obj_item(index)
    if index in self.feature_map:
      return image, self.feature_map[index]['f_v']
    else:
      feature_vector = Utilities.get_feature_vector_for_image(label_list)
      self.feature_map[index] = {}
      self.feature_map[index]['f_v'] = feature_vector
    return image, feature_vector

"""# PyTorch Dataset

Here you will implement a simple pytorch dataset that loads the images and labels as describe in the PDF.
"""

# torch and torchvision imports
import torch
import torchvision
import h5py
import numpy as np

class HW3Dataset(torch.utils.data.Dataset):
  def __init__(self, fl, mode="Train"):
    self.fl = fl
    self.indexes = []
    
    if mode == "Train":
      self.indexes = fl.train_indexes
    else:
      self.indexes = fl.test_indexes
      
  
  def orig_index(self, index):
    return self.indexes[index]
  
  def __len__(self):
    return len(self.indexes)
  
  def get_item(self, index):
    o_index = self.orig_index(index)
    image, label_list = self.fl.get_obj_item(o_index)
    return image, label_list
  
  def __getitem__(self, index):
#     o_index = self.orig_index(index)
#     return np.zeros((300, 400,3), dtype=float), np.zeros((300, 400), dtype=float),  o_index
    o_index = self.orig_index(index)
    image, feature_vector = self.fl.get_feature_item(o_index)
    feature_vector_modified = Utilities.random_sample_anchors(feature_vector)
    return image, feature_vector, o_index,feature_vector_modified, index

"""# Mask Visualization and reconstruction test

Code to visualize the input image, anchor allocation, sampling and reconstruction.
"""

fl = FullLoader(PATHS)
image, label_list  = fl.get_obj_item(9)
Utilities.plot_image(image.astype(int), label_list)

feat_Vec = Utilities.get_feature_vector_for_image(label_list)
plt.imshow(feat_Vec[:, :, 0])
plt.show()


label_object_list = Utilities.reconstruct_labels(feat_Vec)

refined_list = [label for label in label_object_list if label.prob == 1.0]
Utilities.plot_image(image.astype(int), refined_list)

f_v = Utilities.random_sample_anchors(feat_Vec)
plt.imshow(f_v[:, :, 0])
plt.show()

"""# Histogram Visualization"""

import matplotlib.pyplot as plt
import math

def get_mode_from_data(bins, data):
  
  assignment = np.digitize(data, bins)
  
  mp = {}
  
  for i in range(len(data)):
    bi =  assignment[i]
    if bi not in mp:
      mp[bi] = []
    mp[bi].append(data[i])
  
  keys = list(mp.keys()) 
  mp_count_list = [(key, len(mp[key]), np.mean(mp[key]), np.var(mp[key]), len(mp[key])*1.0/len(data) ) for key in keys]
  mp_count_list.sort(key = lambda x: x[1], reverse=True)
  
  print(mp_count_list[0])
  
  
def visualize_histogram(full_loader):
  
  size = full_loader.sample_size
  scale_aspect_list = []
  
  for i in range(size):
    #print(i)
    boxes = full_loader.boxes[i]
    for box_index in range(boxes.shape[0]):
      box = boxes[box_index]
      w = box[2] - box[0]
      h = box[3] - box[1]
      scale = w * h
      aspect_ratio = w / h
      scale_aspect_list.append([aspect_ratio, scale])
        
  scale_only_list = [x[1] for x in scale_aspect_list]
  aspect_only_list = [x[0] for x in scale_aspect_list]
  
  for num_bins in range(1, 11):
    num_bins = num_bins * 10
    print(num_bins)
    n, bins, patches = plt.hist(scale_only_list, num_bins, facecolor='blue', alpha=0.5)

    plt.title('Scale Histogram ' +  "# bins = " + str(num_bins))
    plt.savefig(SAVE_FOLDER_PATH + 'Scale_Histogram_' + str(num_bins) + '.png')
    plt.show()


    get_mode_from_data(bins, scale_only_list)

    n, bins, patches = plt.hist(aspect_only_list, num_bins, facecolor='blue', alpha=0.5)  
    plt.title('Aspect ratio Histogram'  +  "# bins = " + str(num_bins))
    plt.savefig(SAVE_FOLDER_PATH + 'Aspect_ratio_Histogram_' + str(num_bins) + '.png')
    plt.show()

    get_mode_from_data(bins, aspect_only_list)
  
visualize_histogram(fl)

"""# Model Definition"""

import torch
import torch.nn as nn

class BaseNetwork(torch.nn.Module):
  def __init__(self):
    super().__init__()
    conv1 = nn.Conv2d(3, 16, 5, padding=2)
    
    conv1_seq = nn.Sequential(conv1, nn.BatchNorm2d(num_features = 16),
                             nn.ReLU())
    
    pool1 = nn.MaxPool2d(2, stride=2, padding = 0)
    
    
    conv2 = nn.Conv2d(16, 32, 5, padding=2)
    
    conv2_seq = nn.Sequential(conv2, nn.BatchNorm2d(num_features = 32),
                             nn.ReLU())
    
    pool2 = nn.MaxPool2d(2, stride=2, padding = 0)
    
    
    conv3 = nn.Conv2d(32, 64, 5, padding=2)
    
    conv3_seq = nn.Sequential(conv3, nn.BatchNorm2d(num_features = 64),
                             nn.ReLU())
    
    pool3 = nn.MaxPool2d(2, stride=2, padding = 0)
    
    
    conv4 = nn.Conv2d(64, 128, 5, padding=2)
    
    conv4_seq = nn.Sequential(conv4, nn.BatchNorm2d(num_features = 128),
                             nn.ReLU())
    
    pool4 = nn.MaxPool2d(2, stride=2, padding = 0)
    
    conv5 = nn.Conv2d(128, 256, 5, padding=2)
    
    conv5_seq = nn.Sequential(conv5, nn.BatchNorm2d(num_features = 256),
                             nn.ReLU())
    
    self.base_network = nn.Sequential(conv1_seq, pool1, conv2_seq,
                                      pool2, conv3_seq, pool3,
                                      conv4_seq, pool4, conv5_seq)
    
  def forward(self, X):
    return self.base_network.forward(X)
  

class Intermediate_Network(torch.nn.Module):
  def __init__(self, base_model):
    super().__init__()
    self.base_model = base_model
    layer1 = nn.Conv2d(256, 256, 3, padding=1)
    self.layer1_seq = nn.Sequential(layer1, nn.BatchNorm2d(num_features = 256),
                             nn.ReLU())
    
  @staticmethod  
  def from_static_init(base_model, layer1_seq):
    model = Intermediate_Network(base_model)
    model.layer1_seq = layer1_seq
    return model
    
  def forward(self, X):
    base_output = self.base_model.forward(X)
    return self.layer1_seq.forward(base_output)
  
  
class ClassificationNetwork(torch.nn.Module):
  def __init__(self, intermediate_model): 
    super().__init__()
    self.intermediate_model = intermediate_model
    layer2 = nn.Conv2d(256, 1, 1)
    sigmoid = nn.Sigmoid()   
    self.classification_network = nn.Sequential(layer2, sigmoid)
  
  @staticmethod  
  def from_static_init(intermediate_model, classification_network):
    model = ClassificationNetwork(intermediate_model)
    model.classification_network = classification_network
    return model
    
  def forward(self, X):
    intermediate_output = self.intermediate_model.forward(X)
    return self.classification_network.forward(intermediate_output)  
  
  
class RegressionNetwork(torch.nn.Module):
  def __init__(self, intermediate_model): 
    super().__init__()
    self.intermediate_model = intermediate_model
    layer2 = nn.Conv2d(256, 4, 1)
    self.regression_network = nn.Sequential(layer2)
  
  @staticmethod  
  def from_static_init(intermediate_model, regression_network):
    model = RegressionNetwork(intermediate_model)
    model.regression_network = regression_network
    return model
    
  def forward(self, X):
    intermediate_output = self.intermediate_model.forward(X)
    return self.regression_network.forward(intermediate_output)
  


## Sample Initialization.   
base_model = BaseNetwork()
intermediate_model = Intermediate_Network(base_model)

c_model = ClassificationNetwork(intermediate_model)
r_model = RegressionNetwork(intermediate_model)


X = torch.zeros((2,3,300,400), dtype=torch.float)

ans = intermediate_model.forward(X)
print(ans.shape)

ans = c_model.forward(X)
print(ans.shape)

ans = r_model.forward(X)
print(ans.shape)

"""# Loss Function"""

import torch 
import torch.nn.functional as F
import torch.nn as nn

def regression_loss_function(gt, pred, gt_mask_with_out_sampling):
  
  positive_mask = (gt_mask_with_out_sampling[:, :, :, 0] == 1.0).type(torch.float)
    
  scaled_positive_mask = torch.stack([positive_mask] * 4, 3)
  
  loss_value = F.smooth_l1_loss(gt[:, :, :, 1:5], pred[:, :, :, 0:4], reduction="none")
    
  l1_smooth_value = loss_value * scaled_positive_mask
    
  mask_shape = gt_mask_with_out_sampling.shape
    
  flatten_mask = gt_mask_with_out_sampling.reshape((mask_shape[0],-1))
        
  n_anchors_per_batch = torch.sum(flatten_mask.ne(-1), dim=1)
      
  l1_smooth_value_flatten =  l1_smooth_value.reshape((mask_shape[0], -1))
    
  l1_loss_per_batch = torch.sum(l1_smooth_value_flatten, dim=1)
                  
  normed_loss_per_batch = l1_loss_per_batch / (n_anchors_per_batch + 0.0001)
        
  return torch.sum(normed_loss_per_batch)
                     

## loss func reduction is set to none
def classification_loss_function(gt, pred):  
  
  loss = nn.BCELoss(reduction='none')
  
  gt_p_layer = gt[:, :, :, 0]
  
  bce_gt_copy = gt_p_layer.clone()
  bce_gt_copy[gt_p_layer == -1.0] = 0.0 
     
  positive_mask = (gt[:, :, :, 0] == 1.0).type(torch.float)
  negative_mask = (gt[:, :, :, 0] == 0.0).type(torch.float)
  
  bce_loss = loss(pred[:,:,:,0], bce_gt_copy)
    
  mask, mask_shape  = gt[:, :, :, 0], gt[:,:,:,0].shape
    
  total_loss = positive_mask * bce_loss + negative_mask * bce_loss
  
  total_loss_flatten = total_loss.reshape((mask_shape[0], -1))
  
  mask_flatten = mask.reshape((mask_shape[0], -1))
  
  loss_per_batch = torch.sum(total_loss_flatten, dim=1)
  
  n_anchors_per_batch = torch.sum(mask_flatten.ne(-1), dim=1) # 256 in general 
    
  normed_loss_per_batch = loss_per_batch / n_anchors_per_batch
  
  return torch.sum(normed_loss_per_batch)

## Test loss function 

inp = torch.zeros((1,2,2, 5), dtype=torch.float)
pred = torch.zeros((1,2,2, 4), dtype=torch.float)

inp_with_out_sample = torch.zeros((1, 2,2,1), dtype=torch.float)

inp_with_out_sample[0, 0, 0, 0] = 1.0
inp_with_out_sample[0, 0, 1, 0] = -1.0
inp_with_out_sample[0, 1, 0, 0] = 1.0
inp_with_out_sample[0, 1, 1, 0] = 1.0


inp[0, 0, 0, 0] = 0.0
inp[0, 0, 1, 0] = 0.0
inp[0, 1, 0, 0] = 0.0
inp[0, 1, 1, 0] = 0.0


inp[0, 0, 0, 1] = 1.0
inp[0, 0, 0, 2] = 1.0
inp[0, 0, 0, 3] = 1.0
inp[0, 0, 0, 4] = 1.0

pred[0, 0, 0, 0] = 0.0
pred[0, 0, 0, 1] = 0.5
pred[0, 0, 0, 2] = 0.75
pred[0, 0, 0, 3] = 1.0 


regression_loss_function(inp, pred, inp_with_out_sample)

"""# NMS"""

'''
Params: 
label_list : a list of Label objects. Note all labels must belong to same class. 


Functionality: return the NMS labels. Iteratively selects the max probability bbox and removes the overlapping boxes.

Called by non_maximal_supression to supress class-wise labels.

'''

def non_maximal_supression_per_class(label_list):
  
  label_list.sort(key = lambda x: x.prob, reverse=True)
  
  ans = []
  
  while len(label_list) > 0:
    top = label_list[0]
    label_list = label_list[1:]
    
    ans.append(top)
    
    next_itr = []
    
    for label in label_list:
      iou = IOU_Calculator(label.bbox, top.bbox)
      if iou < NMS_IOU_LIMIT:
        next_itr.append(label)
        
    label_list = next_itr
  
  return ans
  
  
'''
Params: 
label_list : a list of Label objects. 
Functionality: return the NMS labels. Iteratively selects the max probability bbox and removes the overlapping boxes.

'''  


def non_maximal_supression(label_list, return_th_labels=False):
  
  ## filter low conf boxes
  label_list = [i for i in label_list if i.prob >= NMS_PROB_LIMIT]
    
  label_map = {}
  
  for label_obj in label_list:
    cl = label_obj.label
    if cl not in label_map:
      label_map[cl] =[]
    label_map[cl].append(label_obj)
    
  
  ans_map = {}
  
  ans = []
    
  
  keys = label_map.keys()
  for key in keys:
    ans_map[key] = non_maximal_supression_per_class(label_map[key])
    ans += ans_map[key]
    
  if return_th_labels:
    return ans, label_list
  
  return ans

"""# mAP"""

import bisect
'''
Params: 
loader, indexes_list, predictions_list [These are generated in Model Test function]

predictions_list should be numpy

Classname to calculate class-wise mAP. 

returns mAP score
'''

def calculate_map(data_set, indexes_list, predictions_list,  plot_pr_curve=False):
      
  full_index_list = []
  full_prediction_list = []
  
  
  for index in indexes_list:
    full_index_list+= list(index)
   
  for pred in predictions_list:
    full_prediction_list+=list(pred)
  
  
  pair_list = []
  false_positve = []
  false_negative = []
  
  total_gt_len = 0
  
  for i in range(len(full_index_list)):
    
    sample_index = full_index_list[i]
    prediction = full_prediction_list[i]
    orig_index = data_set.indexes[sample_index]
    
    
    img, gt_label_list = data_set.get_item(sample_index)
    
    pred_label_list = Utilities.reconstruct_labels(prediction)
    
    pred_label_list = non_maximal_supression(pred_label_list)
    
    pred_label_list.sort(key=lambda x: x.prob, reverse=True)
    
    pred_label_list = pred_label_list[:5]
      
    gt_copy_list = gt_label_list.copy()
    
    total_gt_len += len(gt_copy_list)
    
    for pred_label in pred_label_list:
      iou_values = [(IOU_Calculator(gt_copy_list[i].bbox, pred_label.bbox),i) for i in range(len(gt_copy_list))]
      
      iou_values.sort(key=lambda x: x[0], reverse=True)
      
      flag = False
      gtl = None
      for k in iou_values:
        if (k[0] >= IOU_T):
          gtl = gt_copy_list[k[1]]
          flag = True
          break
      
      if flag:
        pair_list.append((sample_index, gtl, pred_label))
        del gt_copy_list[k[1]]
        
      else:
        false_positve.append((sample_index, pred_label))
    
    for gtl in gt_copy_list:
      false_negative.append(gtl)
    
  fpr = [x[1].prob for x in false_positve]
  
  fpr.sort()
  
  inverse_sorted = sorted([x[2].prob for x in pair_list])
  
  pair_list.sort(key=lambda x: x[2].prob, reverse = True)
  
  level = 0
  
  de = {}
  
  for level in range(len(pair_list)):
    
    pair = pair_list[level]
    tp = len(inverse_sorted) - bisect.bisect_left(inverse_sorted, pair[2].prob)
    fn = total_gt_len - tp
    fp = len(fpr) - bisect.bisect_left(fpr, pair[2].prob)
    
    prec = tp * 1.0 / (tp + fp)
    rec = tp * 1.0 / (tp + fn)
    
    #print(prec, rec, tp, fp, fn)
    
    if rec not in de:
      de[rec] = prec
    else:
      de[rec] = max(de[rec], prec)
  
  keys = list(de.keys())
  keys.sort(reverse=True)
  val = [de[key] for key in keys]
  
  
  run_max = -1000000
  
  for i in range(len(val)):
    run_max = max(run_max, val[i])
    val[i] = run_max
    
    
  keys.reverse()
  val.reverse()
  
  
  buckets = list(np.arange(0, 1.01, 0.1))
  
  
  map_arr = []
  for buc in buckets:
    split_index = bisect.bisect_left(keys, buc)
    if split_index >= 0 and split_index < len(keys):
      map_arr.append(val[split_index])
    elif split_index == len(keys):
      gg = len(inverse_sorted) + len(fpr)
      if gg == 0:
        map_arr.append(gg)
      else:
        map_arr.append(len(inverse_sorted) * 1.0 / ( len(inverse_sorted) + len(fpr)))
    else:
      map_arr.append(0.0)
      

  mAP = sum(map_arr)*1.0/len(map_arr)
  
  print(mAP)
  
  if plot_pr_curve:
    plt.xlim(0, 1.0)
    plt.ylim(0, 2.0)

    plt.xlabel("recall")
    plt.ylabel("precision")
    
    plt.title("PR Curve : All Classes")
   
    plt.plot(buckets, map_arr, label="Quantized")
    plt.show()
  
  return mAP

"""# Network Wrapper"""

from torch.utils.data import DataLoader
import torch.optim as optim
from sklearn.metrics import precision_score,recall_score, accuracy_score


class Epoch_Reader():
  
  def __init__(self, epoch, regression_loss, classification_loss, c_accuracy, prec_total, rec_total, accuracy_total):  
    self.epoch = epoch
    self.regression_loss = regression_loss
    self.classification_loss = classification_loss
    self.c_accuracy = c_accuracy
    self.prec_total, self.rec_total, self.accuracy_total = prec_total, rec_total, accuracy_total

class NetworkWrapper():
  
  def __init__(self, l_r, load_from = None):
    self.l_r = l_r
    
    if load_from is None:
      self.base_model = BaseNetwork().to(device)
      self.intermediate_model = Intermediate_Network(self.base_model).to(device)
      self.c_model = ClassificationNetwork(self.intermediate_model).to(device)
      self.r_model = RegressionNetwork(self.intermediate_model).to(device)
    
    else:
      self.load_network(load_from)
            
    self.regression_optimizer = optim.Adam(list(self.r_model.parameters()) , lr=self.l_r)
    
    self.classifier_optimizer = optim.Adam(list(self.c_model.parameters()) , lr=self.l_r)
    
    self.common_optimizer = optim.Adam(list(self.intermediate_model.parameters())
                                           + list(self.r_model.regression_network.parameters())
                                           + list(self.c_model.classification_network.parameters()) , lr=self.l_r)
  
  def save_network(self, name):
    ## save base model
    torch.save(self.base_model, name + "_basemodel.pt")
    
    ## 
    torch.save(self.intermediate_model.layer1_seq , name+ "_intermodel.pt")
    
    ##
    torch.save(self.r_model.regression_network, name + "_regmodel.pt")
    
    ## 
    torch.save(self.c_model.classification_network, name + "_classmodel.pt")
    
    ## code to save the optimizer state. 
  
  def load_network(self, name):
    
    self.base_model = torch.load(name + "_basemodel.pt")
    layer1_seq =  torch.load(name + "_intermodel.pt")
    regression_network = torch.load(name + "_regmodel.pt")
    classification_network = torch.load(name + "_classmodel.pt")
    
    
    self.intermediate_model = Intermediate_Network.from_static_init(self.base_model, layer1_seq)
    self.c_model = ClassificationNetwork.from_static_init(self.intermediate_model, classification_network)
    self.r_model = RegressionNetwork.from_static_init(self.intermediate_model, regression_network)
    
    ## load the optimizer state.
    
  
  def model_to_train(self):
    self.base_model.train()
    self.intermediate_model.train()
    self.c_model.train()
    self.r_model.train()
    
  def model_to_eval(self):
    self.base_model.eval()
    self.intermediate_model.eval()
    self.c_model.eval()
    self.r_model.eval()
    
  def test_sample(self, sample):
    self.model_to_eval()
    r_prediction, c_prediction = None, None
    with torch.no_grad():
      sample = sample.type(torch.float)
      sample = sample.to(device)
      
      r_naive_prediction = self.r_model(sample)
      r_prediction = r_naive_prediction.permute(0,2,3,1)
        
      c_naive_prediction = self.c_model(sample)
      c_prediction = c_naive_prediction.permute(0,2,3,1)
    
    return r_prediction, c_prediction
      
    
  def test_network(self, test_loader, epoch_number, prefix=""):
    self.model_to_eval()
    
    r_total_loss = 0
    c_total_loss = 0
    p_w_accuracy = 0
    
    
    count = 0
    anchor_count = 0
    
    index_list = []
    prediction_list = []
    
    
    gt_point_wise_list = [0]
    pred_point_wise_list = [0]
    pred_point_wise_prob_list = [0]
    
    
    with torch.no_grad():
      for sample, f_v, o_index, _, index in test_loader:
        sample = sample.type(torch.float)
        sample = sample.to(device)
        f_v = f_v.type(torch.float)
        f_v = f_v.to(device)
        
        r_naive_prediction = self.r_model(sample)
        r_prediction = r_naive_prediction.permute(0,2,3,1)
        
        c_naive_prediction = self.c_model(sample)
        c_prediction = c_naive_prediction.permute(0,2,3,1)
        
                        
        total_prediction = torch.zeros((r_prediction.shape[0], r_prediction.shape[1], r_prediction.shape[2], 5))
        total_prediction[:,:,:,0] = c_prediction[:,:,:,0]
        total_prediction[:,:,:,1:5] = r_prediction[:,:,:,0:4]
        
        r_loss, c_loss = regression_loss_function(f_v, r_prediction, f_v[:,:,:,0:1]), classification_loss_function(f_v, c_prediction)
        
        r_total_loss += r_loss.item() 
        c_total_loss += c_loss.item()
        
        c_rounded = c_prediction.round()
        
        prob_gt = f_v[:,:,:,0:1]
        
        mask = prob_gt.ne(-1)
        
        batch_gt_points = list(prob_gt[mask].cpu().numpy())
        batch_pred_points = list(c_rounded[mask].cpu().numpy())
        batch_pred_prob_list = list(c_prediction[mask].cpu().numpy())
        
        gt_point_wise_list += batch_gt_points
        pred_point_wise_list += batch_pred_points
        pred_point_wise_prob_list += batch_pred_prob_list
        
        match_cords = (f_v[:,:,:,0:1] == c_rounded) * (f_v[:,:,:,0:1].ne(-1))
        
        n_batch_anchors = torch.sum(f_v[:,:,:,0:1].ne(-1).reshape((sample.shape[0], -1)), dim=1)
        
        batch_wise_acc = torch.sum(match_cords.reshape((sample.shape[0], -1)), dim=1)
        
        acc = batch_wise_acc * 1.0 / n_batch_anchors
                
        p_w_accuracy += torch.sum(acc).item()
        
        index_list.append(index)
        prediction_list.append(total_prediction)
         
        count += sample.shape[0]
        
    
    r_average_loss, c_average_loss = r_total_loss/count , c_total_loss/count
    
    p_w_average = p_w_accuracy/count
    
    prec_total = precision_score(gt_point_wise_list, pred_point_wise_list)
    rec_total = recall_score(gt_point_wise_list, pred_point_wise_list)
    accuracy_total = accuracy_score(gt_point_wise_list, pred_point_wise_list)
    
    epoch = Epoch_Reader(epoch_number, r_average_loss, c_average_loss, p_w_average, prec_total, rec_total, accuracy_total)
    
    print(prefix + "Epoch : {},  RLoss: {}, CLoss: {}, Accuracy:{}, Prec: {}, Rec: {}, AccTotal:{}, Count: {}".format(epoch_number+1, r_average_loss, c_average_loss, p_w_average,prec_total,rec_total,accuracy_total,count)) 

    return epoch
    
 
    
  def train_regression(self, train_loader, mini_batch_sample=False):
    self.model_to_train()
    for sample, f_v, o_index, f_v_m, index in train_loader:
      sample = sample.type(torch.float)
      sample = sample.to(device)
      
      f_v = f_v.type(torch.float)
      f_v = f_v.to(device)
      f_v_m = f_v_m.type(torch.float)
      f_v_m = f_v_m.to(device)
      
      self.regression_optimizer.zero_grad()
      
      r_naive_prediction = self.r_model(sample)
      r_prediction = r_naive_prediction.permute(0,2,3,1)
      r_loss = regression_loss_function(f_v, r_prediction, f_v[:,:,:,0:1])
      r_loss.backward()
      self.regression_optimizer.step()
      
      
  def train_classification(self, train_loader, mini_batch_sample=False):
    self.model_to_train()
    for sample, f_v, o_index, f_v_m, index in train_loader:
      sample = sample.type(torch.float)
      sample = sample.to(device)
      f_v = f_v.type(torch.float)
      f_v = f_v.to(device)
      
      f_v_m = f_v_m.type(torch.float)
      f_v_m = f_v_m.to(device)
      
      self.classifier_optimizer.zero_grad()
      
      c_naive_prediction = self.c_model(sample)
      c_prediction = c_naive_prediction.permute(0,2,3,1)
      
      c_loss = classification_loss_function(f_v_m, c_prediction)
      c_loss.backward()
      
      
      self.classifier_optimizer.step()
      
      
  def train_composite(self, train_loader, mult_factor=3):
    
    self.model_to_train()
    for sample, f_v, o_index, f_v_m, index in train_loader:
      sample = sample.type(torch.float)
      sample = sample.to(device)
      f_v = f_v.type(torch.float)
      f_v = f_v.to(device)
      
      f_v_m = f_v_m.type(torch.float)
      f_v_m = f_v_m.to(device)
      
      self.common_optimizer.zero_grad()
      
      r_naive_prediction = self.r_model(sample)
      r_prediction = r_naive_prediction.permute(0,2,3,1)
      
      r_loss = regression_loss_function(f_v, r_prediction, f_v[:,:,:,0:1])
      
      c_naive_prediction = self.c_model(sample)
      c_prediction = c_naive_prediction.permute(0,2,3,1)
      
      c_loss = classification_loss_function(f_v_m, c_prediction)
      
      total_loss = mult_factor* r_loss + c_loss
      
      total_loss.backward()
            
      self.common_optimizer.step()

"""# Loss Plots"""

import matplotlib.pyplot as plt

def plot_training_stats(train_er_list, test_er_list, exp_id="EXP_1"):
    
  epoch_number = [r.epoch+1 for r in train_er_list]
  
  train_avg_loss = [r.regression_loss for r in train_er_list]
  test_avg_loss = [r.regression_loss for r in test_er_list]
  
  train_map = [r.classification_loss for r in train_er_list]
  test_map = [r.classification_loss for r in test_er_list]
  
  
  plt.title("Reg loss over epochs")
  plt.plot(epoch_number, train_avg_loss, label="train_reg_loss")
  plt.plot(epoch_number, test_avg_loss, label="test_reg_loss")
  plt.legend(loc='upper right')
  plt.savefig(SAVE_FOLDER_PATH + 'reg_losscurve_' + exp_id + '.png')

  plt.show()
    
  plt.title("Cls over epochs")
  plt.plot(epoch_number, train_map, label="train_cls_loss")
  plt.plot(epoch_number, test_map, label="test_cls_loss")
  plt.legend(loc='upper right')
  plt.savefig(SAVE_FOLDER_PATH + 'cls_' + exp_id + '.png')
  plt.show()
  
  
  train_total_loss = [2*r.regression_loss + r.classification_loss for r in train_er_list]
  test_total_loss = [2*r.regression_loss + r.classification_loss for r in test_er_list]
  
  plt.title("Total loss over epochs")
  plt.plot(epoch_number, train_map, label="train_total_loss")
  plt.plot(epoch_number, test_map, label="test_total_loss")
  plt.legend(loc='upper right')
  plt.savefig(SAVE_FOLDER_PATH + 'total_' + exp_id + '.png')
  plt.show()
  
  
  train_prec = [r.prec_total for r in train_er_list]
  test_prec = [r.prec_total for r in test_er_list]
  
  train_rec = [r.rec_total for r in train_er_list]
  test_rec = [r.rec_total for r in test_er_list]
  
  train_accuracy_total = [r.accuracy_total for r in train_er_list]
  test_accuracy_total = [r.accuracy_total for r in test_er_list]
  
  plt.title("Precession over epochs")
  plt.plot(epoch_number, train_prec, label="train_prec")
  plt.plot(epoch_number, test_prec, label="test_prec")
  plt.legend(loc='upper right')
  plt.savefig(SAVE_FOLDER_PATH + 'prec_' + exp_id + '.png')
  plt.show()
  
  plt.title("Recall over epochs")
  plt.plot(epoch_number, train_rec, label="train_rec")
  plt.plot(epoch_number, test_rec, label="test_rec")
  plt.legend(loc='upper right')
  plt.savefig(SAVE_FOLDER_PATH + 'rec_' + exp_id + '.png')
  plt.show()
  
  
  plt.title("Accuracy over epochs")
  plt.plot(epoch_number, train_accuracy_total, label="train_acc")
  plt.plot(epoch_number, test_accuracy_total, label="test_acc")
  plt.legend(loc='upper right')
  plt.savefig(SAVE_FOLDER_PATH + 'prec_' + exp_id + '.png')
  plt.show()

"""# Train your network

It might be good to save checkpoints and reload from the most recent. This is due to time constraints inside of colab.
"""

def experiment(nw, epoch_limit, bool_f, mult_factor=3):

  r_train_epoch_list = []
  r_test_epoch_list = []
  
  if bool_f[0]:
    for epoch_num in range(epoch_limit[0]):  
      nw.train_regression(train_data_loader)
      epoch = nw.test_network(train_data_loader, epoch_num, prefix="Regression Train")
      r_train_epoch_list.append(epoch)
      epoch = nw.test_network(test_data_loader, epoch_num, prefix="Regression Test")
      r_test_epoch_list.append(epoch)
    
  c_train_epoch_list = []
  c_test_epoch_list = []
  
  if bool_f[1]:
    for epoch_num in range(epoch_limit[1]):  
      nw.train_classification(train_data_loader)
      epoch = nw.test_network(train_data_loader, epoch_num, prefix="Classification Train")
      c_train_epoch_list.append(epoch)
      epoch = nw.test_network(test_data_loader, epoch_num, prefix="Classification Test")
      c_test_epoch_list.append(epoch)
    
    
  t_train_epoch_list = []
  t_test_epoch_list = []

  if bool_f[2]:
    for epoch_num in range(epoch_limit[2]):  
      nw.train_composite(train_data_loader, mult_factor=mult_factor)
      epoch = nw.test_network(train_data_loader, epoch_num, prefix="Combined Train")
      t_train_epoch_list.append(epoch)
      epoch = nw.test_network(test_data_loader, epoch_num, prefix="Combined Test")
      t_test_epoch_list.append(epoch)
    
  return [(r_train_epoch_list, r_test_epoch_list), (c_train_epoch_list, c_test_epoch_list), (t_train_epoch_list, t_test_epoch_list)]

## Load Data loaders

import pickle

train_data_set =  HW3Dataset(fl)
test_data_set = HW3Dataset(fl, mode='Test')

batch_size = 1
train_data_loader = DataLoader(train_data_set, batch_size = batch_size, shuffle=True)
test_data_loader = DataLoader(test_data_set, batch_size = batch_size, shuffle=True)

"""## Train Regression Network"""

## EXP_ID used for saving network params to disk. 

exp_ID = "TRAIL_X_REG"
nw = NetworkWrapper(0.00001)
train_data = experiment(nw, [30,0,0], bool_f=[True, False, False], mult_factor=2)
pickle.dump(train_data, open(SAVE_FOLDER_PATH + 'train_error_'+ exp_ID, 'wb'))
nw.save_network(SAVE_FOLDER_PATH + "net_" + exp_ID)
plot_training_stats(train_data[0][0], train_data[0][1], exp_id=exp_ID)

"""## Train Classification Network"""

exp_ID = "TRAIL_X_CLS"
nw = NetworkWrapper(0.000001)
train_data = experiment(nw, [0,30,0], bool_f=[False, True, False], mult_factor=2)
pickle.dump(train_data, open(SAVE_FOLDER_PATH + 'train_error_'+ exp_ID, 'wb'))
nw.save_network(SAVE_FOLDER_PATH + "net_" + exp_ID)
plot_training_stats(train_data[1][0], train_data[1][1], exp_id=exp_ID)

"""## Train Total Network"""

exp_ID = "TRAIL_X_TOT"
nw = NetworkWrapper(0.000001)
train_data = experiment(nw, [0,0,30], bool_f=[False, False, True], mult_factor=2)
pickle.dump(train_data, open(SAVE_FOLDER_PATH + 'train_error_'+ exp_ID, 'wb'))
nw.save_network(SAVE_FOLDER_PATH + "net_" + exp_ID)
plot_training_stats(train_data[2][0], train_data[2][1], exp_id=exp_ID)

"""# Test your network

Did you remember to cut out a test set? If not you really should, test on images your network has never seen.
"""

def test_network(nw, test_data_set, index):
  
  image, feature_vector, o_index,feature_vector_modified, _ = test_data_set[index]
  
  print(o_index)
  
  _, orig_label_list  = fl.get_obj_item(o_index)
  
  Utilities.plot_image(image.astype(int), orig_label_list)

  
  batch_test_image = torch.zeros((1, 3, 300, 400)).type(torch.float)
  batch_test_image[0, :, :, :] = torch.tensor(image)

  batch_fv = torch.zeros((1, 18, 25, 5)).type(torch.float)
  batch_fv[0, :, :, :] = torch.tensor(feature_vector)
  
  r_fv, c_fv = nw.test_sample(batch_test_image)
    
  prediction_fv = torch.zeros((1, 18, 25, 5)).type(torch.float)
  prediction_fv[0, :, :, 0] = c_fv[0, :, :, 0]
    
  prediction_fv[0, :, :, 1:5] = r_fv[0, :, :, :]
  
  plt.imshow(feature_vector[:,:,0])
  plt.show()
  
  plt.imshow(c_fv[0, :, :, 0].cpu().numpy(), vmin=-1)
  plt.show()
  
  labels = Utilities.reconstruct_labels(prediction_fv[0,:,:,:].cpu().numpy())
      
  ## label threshold 
  
  labels.sort(key=lambda x: x.prob, reverse=True)
  
  prob_th = [label for label in labels if label.prob > 0]
  
  print(len(prob_th))
  
  neg_labels = [label for label in labels if label.prob < 0.2]
  neg_labels.sort(key=lambda x: x.prob)
  for label in neg_labels:
    label.color='r'
    
        
  nms_th = non_maximal_supression(prob_th)
  
  nms_th.sort(key=lambda x: x.prob, reverse=True)
  
  print(len(nms_th))
  
  nms_th = nms_th[:2]
  
    
  Utilities.plot_image(image.astype(int), nms_th)
  
  Utilities.plot_image(image.astype(int), prob_th[:5] + neg_labels[:5])
  
  

## Load the saved model. It is net_ + "exp_id" used during training.   
pq = NetworkWrapper(0.001, load_from = SAVE_FOLDER_PATH + "net_TRAIL_2_NO_Over_MIX")
test_network(pq, test_data_set, 103)

