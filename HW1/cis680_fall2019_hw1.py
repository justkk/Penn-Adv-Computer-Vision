# -*- coding: utf-8 -*-
"""CIS680_Fall2019_HW1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V2q0YGqhB8X7GiohZDimwd9HCbek9SCG

# Google Drive

This first code block attaches your google drive and makes a folder structure. You only need to run this when a new VM is assigned to you. To get your code as a single python file go through the following menus File->'Download .py'
"""

import os
from google.colab import drive

# Mount google drive
DRIVE_MOUNT='/content/gdrive'
drive.mount(DRIVE_MOUNT, force_remount=True)


# create folder to write data to
CIS680_FOLDER=os.path.join(DRIVE_MOUNT, 'My Drive', 'CIS680_2019')
HOMEWORK_FOLDER=os.path.join(CIS680_FOLDER, 'HW1')
os.makedirs(HOMEWORK_FOLDER, exist_ok=True)

## Create folders to save images and plots.
os.makedirs(os.path.join(HOMEWORK_FOLDER, 'P1'), exist_ok=True)
os.makedirs(os.path.join(HOMEWORK_FOLDER, 'P2'), exist_ok=True)
os.makedirs(os.path.join(HOMEWORK_FOLDER, 'P3'), exist_ok=True)
os.makedirs(os.path.join(HOMEWORK_FOLDER, 'P4'), exist_ok=True)
os.makedirs(os.path.join(HOMEWORK_FOLDER, 'ADV1'), exist_ok=True)
os.makedirs(os.path.join(HOMEWORK_FOLDER, 'ADV2'), exist_ok=True)

"""# PyTorch

This code block imports torch and torchvision which provide the libraries for use down below. PyTorch provides many utilities such as standard datasets, for this exercise we will use MNIST. Please see the torchvision documentation for how to instantiate the training and testing data split.
"""

# torch and torchvision imports
import torch
import torchvision

# Download MNIST
#torchvision.datasets.MNIST('.', download=True)

"""# 1)  Plot Loss and Gradient"""

import torch
import math
import numpy as np
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
from matplotlib import cm
from matplotlib.ticker import LinearLocator, FormatStrFormatter
import numpy as np


    
class SingleNeuron():

    def getSigmoidValue(self, x):
        return 1.0 / (1 + np.exp(-x))

    def __init__(self, input, output):
        self.input = input
        self.output = output

    def plot_utility(self, X, Y, Z, title):
        fig = plt.figure()
        ax = fig.gca(projection='3d')
        surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False)
        ax.set_xlabel('Weights')
        ax.set_ylabel('Bias')
        ax.set_zlabel('Output')
        fig.colorbar(surf, shrink=0.5, aspect=5)
        plt.title(title)
        plt.savefig("./gdrive/My Drive/CIS680_2019/HW1/P1/prob1_" + title +".png")
        
        

    def get_neuron_activation(self, X, Y):
        return self.getSigmoidValue(X*self.input + Y)


    def compute_vals(self, weightRange, biasRange, functionName):

        X, Y = np.meshgrid(weightRange, biasRange)
        Z = self.get_neuron_activation(X, Y)

        response = None 

        if functionName == "sigmoid":
            response = Z

        elif functionName == "squared_loss":
            response = np.power(Z - self.output, 2)

        elif functionName == "grad_squared_loss":
            response = Z * (1-Z) * self.input * 2 * (Z - self.output)

        elif functionName == "cross_entropy_loss":
            response = -1 * (self.output * np.log(Z) + (1 - self.output) * np.log(1 - Z))

        elif functionName == "grad_cross_entropy_loss":
            response = -1 * (self.output/Z - (1-self.output)/(1 - Z))*Z * (1-Z)*self.input

        return X, Y, response

    def plot(self, weightRange, biasRange, functionName):
        X, Y, Z = self.compute_vals(weightRange, biasRange, functionName)
        self.plot_utility(X, Y, Z, functionName)


        
## Code to generate plots for Prob1.

sn = SingleNeuron(1, 0.5)

## Init weight and bias. 
weightRange =  np.arange(-5, 5, 0.25)
biasRange = np.arange(-5, 5, 0.25)

## Plot function surface. 
sn.plot(weightRange, biasRange, "sigmoid")
sn.plot(weightRange, biasRange, "squared_loss")
sn.plot(weightRange, biasRange, "grad_squared_loss")
sn.plot(weightRange, biasRange, "cross_entropy_loss")
sn.plot(weightRange, biasRange, "grad_cross_entropy_loss")

"""# 2) Solving XOR with a 2-layer Perceptron"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import torch.optim as optim

import random

# seed = 7
# torch.backends.cudnn.deterministic = True
# torch.backends.cudnn.benchmark = False
# torch.manual_seed(seed)
# torch.cuda.manual_seed_all(seed)
# np.random.seed(seed)
# random.seed(seed)



activation_functions = {}
activation_functions["sigmoid"] = torch.nn.Sigmoid()
activation_functions["relu"] = torch.nn.ReLU()
activation_functions["tan"] = torch.nn.Tanh()
activation_functions["linear"] = torch.nn.Softshrink(lambd = 0)


loss_functions = {}
loss_functions["squared_error"] = torch.nn.MSELoss(reduction='sum', size_average=False)
loss_functions["cross_entropy"] = torch.nn.BCELoss(size_average=True)

g_counter = 0


def get_activation_function(function_name):
  if function_name in activation_functions:
    return activation_functions[function_name]  
  raise ValueError("Invalid activation function")


def get_loss_function(function_name):
  if function_name in loss_functions:
    return loss_functions[function_name]
  raise ValueError("Invalid loss function")

  
def plot_point(plt, cords, label, color, fc):
  plt.plot([cords[0]], cords[1], color)
  plt.text(cords[0], cords[1], label, size=10, rotation=30.,
         ha="center", va="center",
         bbox=dict(boxstyle="round", ec=(1., 0.5, 0.5), fc=fc, alpha=0.3))
  
  
  
def display_state(transformed_input, line_params, epoch=0):
  
  plot_point(plt,transformed_input[0][:], "(0,0)", 'ro', "blue")
  plot_point(plt,transformed_input[1][:], "(0,1)", 'gs', "yellow")
  plot_point(plt,transformed_input[2][:], "(1,0)", 'gs', "yellow")
  plot_point(plt,transformed_input[3][:], "(1,1)", 'ro', "blue")
  
  plt.xlabel("x-axis")
  plt.ylabel("y-axis")

  sim_line_x = np.arange(-10, 10, 0.01)
  sim_line_y = -1*(sim_line_x * line_params[0][0,0] + line_params[1])/line_params[0][0,1]
  
  plt.plot(sim_line_x, sim_line_y, "r-")

  x_lim = np.max(transformed_input[:,0]) - np.min(transformed_input[:,0])
  plt.xlim(np.min(transformed_input[:,0]) - x_lim/2, np.max(transformed_input[:,0]) + x_lim/2)

  y_lim = np.max(transformed_input[:,1]) - np.min(transformed_input[:,1])
  plt.ylim(np.min(transformed_input[:,1]) - y_lim/2, np.max(transformed_input[:,1]) + y_lim/2)
    
  plt.title("Decision Boundary")
  plt.savefig("./gdrive/My Drive/CIS680_2019/HW1/P2/prob2_" + str(epoch) +".png")
  plt.show()
  
  

class Perceptron():
  
  def __init__(self, params):
    self.input_dim = params["input_dim"]
    self.output_dim = params["output_dim"]
    self.batch_size = params["batch_size"]
    self.learning_rate = params["learning_rate"]
    self.loss_type = params["loss_type"]
    self.loss_function = get_loss_function(self.loss_type)
    self.activation_type = params["activation_type"]
    self.activation_function = get_activation_function(self.activation_type)
    self.weight_init_bias = params["weight_init_bias"]
    self.weight_init_stdev = params["weight_init_stdev"]
    cuda_available = params["cuda_available"]
  
    self.device = self.device = torch.device("cpu")
    
    
    
    
    self.layer_1 = torch.nn.Linear(self.input_dim, 2)
    self.layer_2 = torch.nn.Linear(2, 1)
    
    self.layer_1.weight.data.uniform_(-1.0* self.weight_init_stdev, self.weight_init_stdev)
    self.layer_1.bias.data.fill_(self.weight_init_bias)
    
    self.layer_2.weight.data.uniform_(-1.0*self.weight_init_stdev, self.weight_init_stdev)
    self.layer_2.bias.data.fill_(self.weight_init_bias)
    
    self.sigmoid = torch.nn.Sigmoid()
    
    self.model = torch.nn.Sequential(self.layer_1, self.activation_function,
                                     self.layer_2, self.sigmoid)
    
    self.optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate, momentum = 0.9)
    
    if cuda_available:
      self.device = torch.device("cuda")
      self.model = self.model.to(self.device) 
    
    
  
  def get_network_state(self, input):
    o_1 = self.layer_1(input)
    a_1 = self.activation_function(o_1)
    o_2 = self.layer_2(a_1)
    s_f = self.sigmoid(o_2)
    weights = self.layer_2.weight.cpu().data.numpy()
    bias = float(self.layer_2.bias.cpu().data.numpy())
    return np.copy(a_1.cpu().data.numpy()), (np.copy(weights), np.copy(bias))
  
  
  def train(self, inputs, expected_outputs, epoch_limit):
    
    print(expected_outputs)
    
    inputs_torch = torch.from_numpy(inputs).float()
    outputs_torch = torch.from_numpy(expected_outputs).float()
    
    inputs_torch = inputs_torch.to(self.device)
    outputs_torch = outputs_torch.to(self.device)
    
    
    transformed_input, line_params = self.get_network_state(inputs_torch)
    display_state(inputs_torch.cpu().numpy(), line_params)
    
    
    accuracies = []
    losses = []
    line_params = []
    
    
    for epoch in range(epoch_limit):
      
      output_pred = self.model(inputs_torch)
      loss = self.loss_function(output_pred, outputs_torch)
      
      labels_pred = output_pred.cpu().data.numpy() > 0.5
      labels_pred = labels_pred.astype("int")     
      accuracy = np.sum(labels_pred == expected_outputs) * 1.0/ labels_pred.shape[0]
      

      if len(accuracies) > 0 and accuracy != accuracies[-1]:
        transformed_input, line_params = self.get_network_state(inputs_torch)
        display_state(transformed_input, line_params, epoch=epoch)
        pass
      
      self.model.zero_grad()
      loss.backward()
      self.optimizer.step()
#       with torch.no_grad():
#         for param in self.model.parameters():
#           param -= self.learning_rate * param.grad
        
      losses.append(float(loss.cpu().data.numpy()))
      accuracies.append(accuracy)
      
      print("###############################")
      print("Epoch : ", epoch)
      print("Loss : ", losses[-1])
      print("Accuracy : ", accuracies[-1])
      print("###############################")
      
      
      if accuracies[-1] == 1.0:
        break
    
    return accuracies, losses
  
def plot_accuracy_loss(accuracies, losses, title_pre):
  
  x_len = len(accuracies)
  x_values = range(x_len)
  
  fig = plt.figure()
  ax = plt.axes()
  plt.title("_".join([title_pre, "Accuracy"]) )
  plt.plot(x_values, accuracies, 'r-', label='Accuracy')
  plt.legend(loc='lower right')
  plt.savefig("./gdrive/My Drive/CIS680_2019/HW1/P2/prob2_Accuracy.png")
  plt.show()


  fig = plt.figure()
  ax = plt.axes()
  plt.title("_".join([title_pre, "Loss"]) )
  plt.plot(x_values, losses, 'r-', label='Loss')
  plt.legend(loc='upper right')
  plt.savefig("./gdrive/My Drive/CIS680_2019/HW1/P2/prob2_Loss.png")
  plt.show()
  
def runner(inputs, outputs, activation_name, loss_name):
  
  params = {}
  params["input_dim"] = 2
  params["output_dim"] = 1
  params["batch_size"] = 4
  params["learning_rate"] = 0.1
  params["loss_type"] = loss_name
  params["activation_type"] = activation_name
  params["weight_init_bias"] = 0.1
  params["weight_init_stdev"] = 0.1
  params["cuda_available"] = False
  
  network = Perceptron(params)
  accuracies, losses = network.train(inputs, outputs.reshape(outputs.shape[0], 1), 10000)
  plot_accuracy_loss(accuracies, losses, "_".join([activation_name, loss_name]))


## XOR outputs. 
outputs = np.array([0,1,1,0])

# Tan : LR: 0.05
# Sigmoid: 0.005

## XOR Inputs. 
inputs = np.zeros((4, 2))
inputs[0, :] = np.array([0,0])
inputs[1, :] = np.array([0,1])
inputs[2, :] = np.array([1,0])
inputs[3, :] = np.array([1,1])

## Train perceptron. 
runner(inputs, outputs, "tan", "cross_entropy")

"""# 3) Train a Convolutional Neural Network

### 3.1) CNN and helper classes
"""

import torch
import torch.nn as nn
import torch.nn.functional as func
import torch.optim as optim
import torchvision.datasets as datasets
import torchvision.transforms as transforms

from torch.utils.data import DataLoader
import numpy as np
from functools import reduce
import random

# seed = 1
# torch.backends.cudnn.deterministic = True
# torch.backends.cudnn.benchmark = False
# torch.manual_seed(seed)
# torch.cuda.manual_seed_all(seed)
# np.random.seed(seed)
# random.seed(seed)


class Flatten(nn.Module):
    def forward(self, input):
        return input.view(input.size(0), -1)

# Create your network here
class BasicCNN(torch.nn.Module):    
  
  def __init__(self):
    super(BasicCNN, self).__init__()
    
    ### params
    self.conv1 = nn.Conv2d(1, 32, 5, padding=2)
    self.conv2 = nn.Conv2d(32, 32 , 5, padding=2)
    self.conv3 = nn.Conv2d(32, 64, 5, padding=2)
    
    self.fc1 = nn.Linear(576, 64)
    self.fc2 = nn.Linear(64, 10)
    
    self.conv1_seq = nn.Sequential(self.conv1, nn.BatchNorm2d(num_features = 32),
                             nn.ReLU())
    
    self.conv2_seq = nn.Sequential(self.conv2, nn.BatchNorm2d(num_features = 32),
                             nn.ReLU())
    
    self.conv3_seq = nn.Sequential(self.conv3, nn.BatchNorm2d(num_features = 64),
                             nn.ReLU())
    
    self.conv_seq = nn.Sequential(self.conv1_seq,
                             nn.AvgPool2d((2,2), stride=2, padding=0),
                             self.conv2_seq,
                             nn.AvgPool2d((2,2), stride=2, padding=0),
                             self.conv3_seq,
                             nn.AvgPool2d((2,2), stride=2, padding=0),
                            )
    ## TODO Removing softmax
    self.fc_seq = nn.Sequential(self.fc1, nn.BatchNorm1d(num_features = 64), nn.ReLU(), self.fc2)
  
  def forward(self, inp):
    inp = self.conv_seq.forward(inp)
    inp = inp.view(-1, reduce(lambda x, y: x * y, inp.shape[1:], 1))
    inp = self.fc_seq.forward(inp)
    return inp

class EpochRecorder():
  
  def __init__(self, epoch, avg_loss, total_loss, accuracy):
    self.avg_loss = avg_loss
    self.total_loss = total_loss
    self.accuracy = accuracy
    self.epoch = epoch 

  
class NetworkWrapper():
  
  def save_model(self, name):
    #self.model.save_state_dict(name + '.pt')
    torch.save(self.model, name)

    
  def __init__(self, batch_size, cuda_available, learning_rate, load_from=None, adv_loader = None):
    
    self.model = BasicCNN()
    if load_from is not None:      
      self.model= torch.load(load_from)
      
    self.params = {}
    self.device = torch.device("cpu")
    
    self.batch_size = batch_size
    self.learning_rate = learning_rate
    
    if cuda_available:
      self.device = torch.device("cuda")
      self.params["num_workers"] = 0
      self.params["pin_memory"] = True
      self.model = self.model.to(self.device)
    
    self.train_epoch_list = []
    self.test_epoch_list = []
    self.adv_loader = None
    
    ## TODO: Check about -1 to +1 
    self.train_loader = DataLoader(
        datasets.MNIST('./MNIST', train=True, download=True,
                       transform=transforms.Compose([
                           transforms.ToTensor(),
                           transforms.Normalize((0.5,), (0.5,))
                       ])),
        batch_size = batch_size, shuffle=False, **self.params)
    
    
    self.test_loader = DataLoader(
        datasets.MNIST('./MNIST', train=False, download=True,
                       transform=transforms.Compose([
                           transforms.ToTensor(),
                           transforms.Normalize((0.5,), (0.5,))
                       ])),
        batch_size = batch_size, shuffle=False, **self.params)
          
    
    self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
    self.train_entropy_loss = nn.CrossEntropyLoss()
    self.test_entropy_loss = nn.CrossEntropyLoss(reduction="sum")
    pass
    
  def train(self, epoch, train_loader):
    self.model.train()
    total_loss = 0
    correct_classification = 0
    
    epoch_list = []
    
    for batch_id, (sample, label) in enumerate(train_loader):
      sample = sample.to(self.device)
      label = label.to(self.device)
      self.optimizer.zero_grad()
      prediction = self.model(sample)
      loss = self.train_entropy_loss(prediction, label)
      loss.backward()
      self.optimizer.step()
      
    self.model.eval()
    with torch.no_grad():
      for batch_id, (sample, label) in enumerate(train_loader):
        sample = sample.to(self.device)
        label = label.to(self.device)
        prediction = self.model(sample)        
        current_loss = self.test_entropy_loss(prediction, label).item()
        pred_class = prediction.argmax(dim=1, keepdim=True)
        binary_comparision = pred_class.eq(label.view_as(pred_class))
        correct_classification += binary_comparision.sum().item()  
        total_loss += current_loss
      
    
    loss_avg = total_loss/len(train_loader.dataset)
    accuracy = correct_classification/len(train_loader.dataset)
    
    epoch_recorder = EpochRecorder(epoch, loss_avg, total_loss, accuracy)
    
    print("Training: Epoch {}, AVG Loss: {:.8f}, Accuracy: {:.8f}, Total Loss: {:.8f} ".format(epoch, loss_avg, accuracy, total_loss))
    
    return epoch_recorder
  
  
  def test(self, epoch, test_loader):
    ## epoch is -1 for actual testing.
    self.model.eval()
    total_loss = 0
    correct_classification = 0
    with torch.no_grad():
      for batch_id, (sample, label) in enumerate(test_loader):
        sample = sample.to(self.device)
        label = label.to(self.device)
        prediction = self.model(sample)        
        current_loss = self.test_entropy_loss(prediction, label).item()
        pred_class = prediction.argmax(dim=1, keepdim=True)
        binary_comparision = pred_class.eq(label.view_as(pred_class))
        correct_classification += binary_comparision.sum().item()  
        total_loss += current_loss
        
    loss_avg = total_loss/len(test_loader.dataset)
    accuracy = correct_classification/len(test_loader.dataset)
    
    epoch_recorder = EpochRecorder(epoch, loss_avg, total_loss, accuracy)
    print("Testing: Epoch {}, AVG Loss: {:.8f}, Accuracy: {:.8f}, Total Loss: {:.8f} ".format(epoch, loss_avg, accuracy, total_loss))
    return epoch_recorder
    

def plot(epoch_limit, network_wrapper, batch_size, lr, additional_list = [], label_list = []):
  
  x_axis = [i for i in range(1, epoch_limit+1)]
  
  train_avg_loss_axis = [i.avg_loss for i in network_wrapper.train_epoch_list]
  test_avg_loss_axis = [i.avg_loss for i in network_wrapper.test_epoch_list]
  
  
  train_total_loss_axis = [i.total_loss for i in network_wrapper.train_epoch_list]
  test_total_loss_axis = [i.total_loss for i in network_wrapper.test_epoch_list]
  
  train_accuracy_axis = [i.accuracy for i in network_wrapper.train_epoch_list]
  test_accuracy_axis = [i.accuracy for i in network_wrapper.test_epoch_list]
  
  
  import matplotlib.pyplot as plt
  
  plt.title('Experiment : Batch Size {} , Learning Rate {}, Average Loss'.format(batch_size, lr))
  
  plt.plot(x_axis, train_avg_loss_axis, 'bo-', label = "Train Average Loss")
  plt.plot(x_axis, test_avg_loss_axis, 'ro-', label = "Test Average Loss")
  plt.legend()
  plt.ylabel('Cross Entropy Loss')
  plt.xlabel('Epochs')
  plt.savefig("./gdrive/My Drive/CIS680_2019/HW1/P3/prob3_Loss_" + str(batch_size)+"_"+str(lr)+".png")

  plt.show()
  
  
  
  plt.title('Experiment : Batch Size {} , Learning Rate {}, Accuracy'.format(batch_size, lr))
  
  
  plt.plot(x_axis, train_accuracy_axis, 'bo-', label = "Train Accuracy")
  plt.plot(x_axis, test_accuracy_axis, 'ro-', label = "Test Accuracy")
  plt.legend()
  plt.ylabel('Accuracy')
  plt.xlabel('Epochs')  
  plt.savefig("./gdrive/My Drive/CIS680_2019/HW1/P3/prob3_Accuracy_" + str(batch_size)+"_"+str(lr)+".png")
  plt.show()
  
  for (addi, label_name) in zip(additional_list, label_list):
    add_y_axis = [i.avg_loss for i in addi]
    plt.plot(x_axis, add_y_axis, 'bo-', label=label_name)
    plt.show()
  

def experiment(batch_size, cuda_avail, lr, limit):
  # Instantiate your network here
  my_first_network = NetworkWrapper(batch_size, True, lr)

  # Train your network here
  num_epochs = 30
  for epoch in range(num_epochs):
    
    print("Epoch %d/%d" % (epoch+1, num_epochs))
    
    train_epoch_recorder = my_first_network.train(epoch+1, my_first_network.train_loader)
    my_first_network.train_epoch_list.append(train_epoch_recorder) 
        
    test_epoch = my_first_network.test(epoch+1, my_first_network.test_loader)
    my_first_network.test_epoch_list.append(test_epoch)

    if test_epoch.accuracy > limit:
      break  
  plot(epoch+1, my_first_network, batch_size, lr, additional_list=[])
  
  return my_first_network

"""### 3.2) Train a Network."""

net =  experiment(100, True, 0.01, 0.99)
net.save_model("./gdrive/My Drive/CIS680_2019/HW1/prob3.pt")

"""### 3.3) Experiment with hyper-parameters."""

# b_size = [32, 64, 128]
# lr = [0.1, 0.01, 0.001]

# for b in b_size:
#   for l in lr:
#     net =  experiment(b, True, l, 0.999999)

"""### 3.4) Train second model for prob-4"""

# Train second model
net =  experiment(100, True, 0.01, 0.99)
net.save_model("./gdrive/My Drive/CIS680_2019/HW1/prob3_retrain.pt")

"""# 4) Adversarial Images

### 4.1
"""

import matplotlib.pyplot as plt
import random

HARD_TEST = 20

def sign_grad(grad, esp):
  binary = (grad > 0)*2.0 
  binary = binary.float()
  sign = esp * (binary - 1)
  sign = sign.float()
  return sign


def binary_grad(grad, esp):
  binary = (grad > 0)*1.0
  binary = binary.float()
  val = esp * binary
  val = val.float()
  return val
  

def compute_adv_image(img_index, net):
  
  test_image = net.test_loader.dataset[img_index][0]
  test_label = net.test_loader.dataset[img_index][1]
  
  test_label_tensor = torch.tensor(test_label)

  x = test_image.view(1,1,28,28)
  y = test_label_tensor.view(1)

  x = x.to(net.device)
  y = y.to(net.device)

  esp = 0.1
  limit = 0.9
  
  xe = torch.zeros_like(x, dtype=torch.float)

  counter = 1
  x_input = None
  predicted_label = None
  
  loss_plot = []
  
  while(True):
    
    net.optimizer.zero_grad()
    
    xe = xe.detach()
    xe.requires_grad_(True)
    
    x_semi_input = x + torch.clamp(xe, min=0.0)
    x_input = torch.clamp(x_semi_input, min=-1.0, max=1.0)
        
    net.model.eval()
    prediction = net.model(x_input)
    
    current_loss = net.test_entropy_loss(prediction, y)
    current_loss.backward()
    
    loss_plot.append(current_loss.item())
    
    prediction = func.softmax(prediction)
    predicted_label = prediction.argmax(dim=1).item()

    if(predicted_label!= test_label and prediction[0][predicted_label].item() > limit):
      print( "Img_index : {}, Count : {}, Orig: {}, Predicted {}, orig_prob: {:.8f},  prediction_prob: {:.8f}" .format(img_index, counter, test_label, predicted_label, prediction[0][test_label].item() ,prediction[0][predicted_label].item()))
      break
    
    if counter == HARD_TEST:
      print( "$$$ Img_index : {}, Count : {}, Orig: {}, Predicted {}, orig_prob: {:.8f},  prediction_prob: {:.8f}" .format(img_index, counter, test_label, predicted_label, prediction[0][test_label].item() ,prediction[0][predicted_label].item()))
      break
      

    sign = sign_grad(xe.grad, esp)
    xe = xe + sign
    counter += 1
  
  if counter == HARD_TEST:
    return None, None, None, None
  
  x_val = [i+1 for i in range(len(loss_plot))]
  
#   fig = plt.figure()
#   ax = plt.axes()
#   plt.title("_".join([str(img_index), "Loss Plot"]) )
#   plt.plot(x_val, loss_plot, 'r-', label='Loss Plot')
#   plt.legend(loc='lower right')
#   plt.show()
  #noise = torch.clamp(xe.detach().cpu().squeeze(), min=0.0)
  
  original_image = net.test_loader.dataset[img_index][0].squeeze() 
  
  adv_image = x_input.detach().cpu().squeeze()
  
  noise = adv_image - original_image
  
  plt.title("Original Image")
  
  plt.imshow(original_image, cmap='gray', vmin=-1.0, vmax=1.0)
  plt.show()
  
  plt.title("Adversarial Image")
  plt.imshow(adv_image, cmap='gray', vmin=-1.0, vmax=1.0)
  plt.show()
  
  plt.title("Noise")
  plt.imshow(noise, cmap='gray', vmin=0.0, vmax=2.0)
  plt.show()
  
  return adv_image.data.numpy(), original_image.data.numpy(), predicted_label, test_label

    

def generate_adv_image_method_1(net, limit, samples=[]):
  length = len(net.test_loader.dataset)
  sample_set = [i for i in range(length)]
  sample = random.sample(sample_set, limit)
  if len(samples) > 0:
    limit = len(samples)
    sample = samples
  new_data_set = np.zeros((limit, 28, 28), dtype="float")
  sam_orig_pred =  np.zeros((limit, 3), dtype="int")
  counter = 0
  for sample_index in sample:
    if counter%100 == 0:
      print("{}/{}".format(counter+1, limit))
    adv_image, original_image, predicted_label, orig_label = compute_adv_image(sample_index, net)
    
    if adv_image is None:
      continue
    
    new_data_set[counter, :, :] = adv_image
    sam_orig_pred[counter, 0] = sample_index
    sam_orig_pred[counter, 1] = orig_label
    sam_orig_pred[counter, 2] = predicted_label
    counter += 1
  
  return new_data_set[:counter, :, :], sam_orig_pred[:counter, :]
  
  
def save_generated_data(new_data_set, sam_orig_pred, pre, name):
  np.save(pre + "/" + name + "_data_set.npy", new_data_set)
  np.save(pre + "/" + name + "_sam_orig_pred.npy", sam_orig_pred)

def load_generated_data(pre, name):
  new_data_set = np.load(pre + "/" + name + "_data_set.npy")
  sam_orig_pred = np.load(pre + "/" + name + "_sam_orig_pred.npy")
  return new_data_set, sam_orig_pred  
  

def run_4_1():
  net = NetworkWrapper(100, True, 0.1, load_from="./gdrive/My Drive/CIS680_2019/HW1/prob3.pt")  
  new_data_set, sam_orig_pred = generate_adv_image_method_1(net, 3, samples=[4029, 2724, 22])
  save_generated_data(new_data_set, sam_orig_pred, "./gdrive/My Drive/CIS680_2019/HW1", "sim_exp_1")

run_4_1()

"""### 4.2

Note: Change the values
"""

import matplotlib.pyplot as plt
import random


HARD_TEST = 50

def binary_negative_grad(grad, esp):
  binary = (grad < 0)*1.0
  binary = binary.float() * -1.0
  val = esp * binary
  val = val.float()
  return val


def random_target(orig_label, minVal, maxVal):
  rand_inp_array = [i for i in range(minVal, maxVal+1)]
  del rand_inp_array[orig_label]
  return random.choice(rand_inp_array)
  
def compute_adv2_image(img_index, y_target, net):
  
  test_image = net.test_loader.dataset[img_index][0]
  orig_label = net.test_loader.dataset[img_index][1]
  test_label_tensor = torch.tensor(y_target)

  x = test_image.view(1,1,28,28)
  y = test_label_tensor.view(1)

  x = x.to(net.device)
  y = y.to(net.device)

  esp = 0.1
  limit = 0.9
  
  xe = torch.zeros_like(x, dtype=torch.float)
  counter = 1
  while(True):  
    
    net.optimizer.zero_grad()
    xe = xe.detach()
    xe.requires_grad_(True)
        
    x_semi_input = x + torch.clamp(xe, min=0.0)
    x_input = torch.clamp(x_semi_input, min=-1.0, max=1.0)
    
    
    net.model.eval()
    prediction = net.model(x_input)
    current_loss = net.test_entropy_loss(prediction, y)
    current_loss.backward()
    prediction = func.softmax(prediction)
    predicted_label = prediction.argmax(dim=1).item()

    if(prediction[0][y_target].item() > limit):
      print( "Img_index : {}, Count : {}, Target: {}, Predicted {}, target_prob: {:.8f},  orig_prob: {:.8f}" .format(img_index, counter, y_target, predicted_label, prediction[0][y_target].item() ,prediction[0][orig_label].item()))
      break
      
    if counter == HARD_TEST:
      print( "$$$ Img_index : {}, Count : {}, Target: {}, Predicted {}, target_prob: {:.8f},  orig_prob: {:.8f}" .format(img_index, counter, y_target, predicted_label, prediction[0][y_target].item() ,prediction[0][orig_label].item()))
      break

    sign = sign_grad(xe.grad, esp)
    xe = xe - sign
    #xe = xe - esp*torch.clamp(torch.sign(xe.grad).float(), min=-1.0, max=0.0)
    counter += 1
    
  #noise = torch.clamp(xe.detach().cpu().squeeze(), min=0.0)
  if counter == HARD_TEST:
    return None, None, None, None
  
  original_image = net.test_loader.dataset[img_index][0].squeeze() 
  adv_image = x_input.detach().cpu().squeeze()
  
  noise = adv_image - original_image
  
  print("Tesing Output Image .... ... ...")
  
  adv_input = adv_image.view(1,1,28,28)
  adv_input.requires_grad_(False)
  prediction = net.model(x_input)
  soft_max_prediction = func.softmax(prediction)
  print(soft_max_prediction.argmax(dim=1).item())
  
  plt.imshow(original_image, cmap='gray', vmin=-1.0, vmax=1.0)
  plt.show()
  
  plt.imshow(adv_image, cmap='gray', vmin=-1.0, vmax=1.0)
  plt.show()
  
  plt.imshow(noise, cmap='gray', vmin=0.0, vmax=2.0)
  plt.show()
  
  return adv_input.data.numpy(), original_image.data.numpy(), predicted_label, orig_label
  
  

def exp2(img_index, net, target_label=None):
  orig_label = net.test_loader.dataset[img_index][1]
  if target_label is None or target_label == orig_label:
    #print("Setting target_label .....")
    target_label = random_target(orig_label, 0, 9)
    #print("Orig_Label: {}, Test_Label: {}".format(orig_label,target_label))
  
  return compute_adv2_image(img_index, target_label, net)
  
  
def generate_adv_image_method_2(net, limit, samples=[]):
  length = len(net.test_loader.dataset)
  sample_set = [i for i in range(length)]
  sample = random.sample(sample_set, limit)
  if len(samples) > 0:
    limit = len(samples)
    sample = samples
  
  new_data_set = np.zeros((limit, 28, 28), dtype="float")
  sam_orig_pred =  np.zeros((limit, 3), dtype="int")
  counter = 0
  for sample_index in sample:
    if counter%100 == 0:
      print("{}/{}".format(counter+1, limit))
    adv_image, original_image, predicted_label, orig_label = exp2(sample_index, net)
    if adv_image is None:
      continue
    new_data_set[counter, :, :] = adv_image
    sam_orig_pred[counter, 0] = sample_index
    sam_orig_pred[counter, 1] = orig_label
    sam_orig_pred[counter, 2] = predicted_label
    counter += 1
  
  return new_data_set[:counter, :, :], sam_orig_pred[:counter, :]    


def run_4_2():
  net = NetworkWrapper(100, True, 0.01, load_from="./gdrive/My Drive/CIS680_2019/HW1/prob3.pt")  
  new_data_set, sam_orig_pred = generate_adv_image_method_2(net, 3, samples=[4029, 2724, 22])
  
  ## Function call to genrate random 100 images
  #new_data_set, sam_orig_pred = generate_adv_image_method_2(net, 100)
  
  save_generated_data(new_data_set, sam_orig_pred, "./gdrive/My Drive/CIS680_2019/HW1", "sim_exp_2")


run_4_2()

"""### Display image utility"""

def display_image(new_data_set, sam_orig_pred, index, net, folder_path):
  image =  new_data_set[index, :, :]
  label_info = sam_orig_pred[index, :]
  img_index = label_info[0]
  orig_label = label_info[1]
  pred_label = label_info[2]
  test_image = net.test_loader.dataset[img_index][0]
  original_image = test_image.squeeze().numpy() 
  adv_image = image
  noise = adv_image - original_image
  
  fig, (ax1, ax2, ax3) = plt.subplots(1, 3)
  fig.suptitle('Image Index: {}, Original : {}, Target: {}'.format(img_index, orig_label, pred_label))
  ax1.imshow(original_image, cmap="gray", vmin=-1.0, vmax=1.0)
  ax1.set_title("Original Image")
  ax2.imshow(noise, cmap="gray", vmin=0.0, vmax=2.0)
  ax2.set_title("Noise")
  ax3.imshow(adv_image, cmap="gray", vmin=-1.0, vmax=1.0)
  ax3.set_title("Adversarial Image")
  plt.savefig(folder_path + "/" + "adv_" + str(img_index)+"_"+str(orig_label)+"_"+ str(pred_label)+".png") 
  plt.show()

#load saved images
net = NetworkWrapper(100, True, 0.1, load_from="./gdrive/My Drive/CIS680_2019/HW1/prob3.pt") 
new_data_set, sam_orig_pred = load_generated_data("./gdrive/My Drive/CIS680_2019/HW1", "sim_exp_1")

print("########### 4.1#################")
for i in range(0,3):
  display_image(new_data_set, sam_orig_pred, i, net, "./gdrive/My Drive/CIS680_2019/HW1/ADV1/")

print("########### 4.2#################")
new_data_set, sam_orig_pred = load_generated_data("./gdrive/My Drive/CIS680_2019/HW1", "sim_exp_2")

for i in range(0,3):
  display_image(new_data_set, sam_orig_pred, i, net, "./gdrive/My Drive/CIS680_2019/HW1/ADV2/")

"""### Test Adversarial Images"""

import torch.utils.data as utils

def get_data_loader(new_data_set, sam_orig_pred):
  x_input  = list(new_data_set)
  y_input  = list(sam_orig_pred[:,1])

  tensor_x_input = torch.stack([torch.Tensor(i).unsqueeze(0) for i in x_input])
  tensor_y_input = torch.Tensor(y_input).type(torch.LongTensor)

  dataset = utils.TensorDataset(tensor_x_input, tensor_y_input) 
  adv_dataloader = utils.DataLoader(dataset, batch_size = 100, shuffle=False)
  return adv_dataloader

def get_orig_test_loader(new_data_set, sam_orig_pred, test_loader):
  
  x_input = np.copy(new_data_set)
  y_input  = list(sam_orig_pred[:,1])
  
  for i in range(sam_orig_pred.shape[0]):
    x_input[i, :, :] = test_loader.dataset[sam_orig_pred[i][0]][0].squeeze()
  
  tensor_x_input = torch.stack([torch.Tensor(i).unsqueeze(0) for i in x_input])
  tensor_y_input = torch.Tensor(y_input).type(torch.LongTensor)

  dataset = utils.TensorDataset(tensor_x_input, tensor_y_input) 
  test_sample_loader = utils.DataLoader(dataset, batch_size = 100, shuffle=False)
  
  return test_sample_loader
    


new_data_set, sam_orig_pred = load_generated_data("./gdrive/My Drive/CIS680_2019/HW1", "sim_exp_1")
net = NetworkWrapper(100, True, 0.1, load_from="./gdrive/My Drive/CIS680_2019/HW1/prob3_retrain.pt")
adv1_loader = get_data_loader(new_data_set, sam_orig_pred)
test_sample_loader = get_orig_test_loader(new_data_set, sam_orig_pred, net.test_loader)

print("Testing 4.1 on second model")
print("Original .... ")
net.test(0, test_sample_loader)
print("Adversarial .... ")
net.test(0, adv1_loader)

print("")
print("")


new_data_set, sam_orig_pred = load_generated_data("./gdrive/My Drive/CIS680_2019/HW1", "sim_exp_2")
net = NetworkWrapper(100, True, 0.1, load_from="./gdrive/My Drive/CIS680_2019/HW1/prob3_retrain.pt")
adv2_loader = get_data_loader(new_data_set, sam_orig_pred)
test_sample_loader = get_orig_test_loader(new_data_set, sam_orig_pred, net.test_loader)


print("Testing 4.2 on second model")
print("Original .... ")
net.test(0, test_sample_loader)
print("Adversarial .... ")
net.test(0, adv1_loader)

print("")
print("")